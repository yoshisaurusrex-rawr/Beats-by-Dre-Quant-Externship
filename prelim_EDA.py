# -*- coding: utf-8 -*-
"""prelim EDA with cleaned set, and visualizations

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1msRE5VC42EewvUo7kIaXskXQhS6NBy0b
"""

import pandas as pd

from google.colab import files
uploaded = files.upload()

df = pd.read_csv('cleaned_final.csv')

print(df.head())
print(df.tail())

missing = df.isnull().sum()
print("Missing values:\n", missing)

# rechecked after running .dropna()

"""since there's only missing values in columns 'content' and 'author', we're just gonna drop them"""

df.dropna(subset=['author', 'content', 'product_attributes'], inplace=True)

"""moving on to descriptive stats"""

# descriptive stats: rating
print("Mean:", df['rating'].mean())
print("Median:", df['rating'].median())
print("Mode:", df['rating'].mode())
print("Standard Deviation:", df['rating'].std())

# descriptive stats: helpful_count (the amount of people that found the review helpful)
print("Mean:", df['helpful_count'].mean())
print("Median:", df['helpful_count'].median())
print("Mode:", df['helpful_count'].mode())
print("Standard Deviation:", df['helpful_count'].std())

import matplotlib.pyplot as plt
import seaborn as sns

# bar plot

plt.figure(figsize=(10, 6))
sns.histplot(df['rating'], bins=5, kde=False)
plt.title("Distribution of Ratings")
plt.xlabel("Rating")
plt.ylabel("Count")
plt.show()

# plotting review volume and trends

df['date_str'] = df['timestamp'].str.extract(r'(\w+\s+\d{1,2},\s+\d{4})')
df['date'] = pd.to_datetime(df['date_str'], errors='coerce')

# create year-month column
df['year_month'] = df['date'].dt.to_period('M')

reviews_per_month = df.groupby('year_month').size()

# convert to timestamps
reviews_per_month.index = reviews_per_month.index.to_timestamp()

# plot!
plt.figure(figsize=(12, 6))
sns.lineplot(x=reviews_per_month.index, y=reviews_per_month.values, marker="o")

plt.title("Review Volume Over Time")
plt.xlabel("Month")
plt.ylabel("Number of Reviews")
plt.xticks(rotation=45)
plt.show()

# word cloud! weighted by frequency in high vs low rated reviews

high_reviews = df[df['rating'] >= 4]['content'].dropna().astype(str)
low_reviews  = df[df['rating'] <= 2]['content'].dropna().astype(str)

from wordcloud import WordCloud

high_text = " ".join(high_reviews.tolist())
low_text  = " ".join(low_reviews.tolist())

wc_high = WordCloud(width=800, height=400, background_color="white", colormap="Greens").generate(high_text)
wc_low  = WordCloud(width=800, height=400, background_color="white", colormap="Reds").generate(low_text)

plt.figure(figsize=(16, 8))

plt.subplot(1, 2, 1)
plt.imshow(wc_high, interpolation="bilinear")
plt.axis("off")
plt.title("High-Rated Reviews", fontsize=16)

plt.subplot(1, 2, 2)
plt.imshow(wc_low, interpolation="bilinear")
plt.axis("off")
plt.title("Low-Rated Reviews", fontsize=16)

plt.show()

# first, find the unique ID's
unique_ids = df['product_id'].unique().tolist()
print(unique_ids)

brand_map = {
    "B09GJVTRNZ": "JBL",
    "B0BYC52LYP": "Ultimate Ears",
    "B08X4YMTPM": "JBL",
    "B0D6WD2QSQ": "Bose",
    "B0DYB6KMJH": "Sony",
    "B0CVFKZ1LC": "Ultimate Ears",
    "B0BW34LCB8": "Sonos",
    "B09PYVXXW5": "JBL",
    "B09FM6PDHP": "JBL",
    "B09HN594TL": "JLab",
    "B096SV8SJG": "Beats",
    "B0BTZKP1TP": "Jabra",
    "B08ZR5JB9G": "Sony",
    "B0C2F5KD26": "Apple",
    "B0BYPFNW6T": "Bose",
    "B093SLWMS7": "Philips",
    "B0CXL4FQBK": "Skullcandy",
    "B099TJGJ91": "Sony",
    "B0D4SX9RC6": "Bang & Olufsen",
    "B0B44F1GGK": "Sony",
    "B09XXW54QG": "Marshall",
    "B07DD3WBYW": "Ultimate Ears",
    "B07YBN9XXG": "Bose",
    "B0CY6S748H": "Sonos",
    "B01IOD7KB6": "House of Marley",
    "B0CVFM97GD": "Ultimate Ears",
    "B08NLCW9WY": "JBL",
    "B08YRT9T38": "JBL",
    "B08VL5S148": "JBL",
    "B097XX34SL": "Sony",
    "B07PXGQC1Q": "JBL",
    "B0BZ9WMLNQ": "Sony",
    "B0D3JB14QS": "Sony",
    "B0BQPNMXQV": "Jabra",
    "B0C1QWWZR4": "JBL",
    "B0CF7GYNW2": "Samsung",
    "B09CFP6J6D": "Sony",
    "B0BZTCXG6T": "Sony",
    "B0863H1JKB": "JBL",
    "B09JL41N9C": "Sony",
    "B09V9P5Q6W": "Sony",
    "B0BZK2Z2TC": "Sony",
    "B0C345M3T7": "Sony",
    "B0CD2FSRDD": "Bose",
    "B0CPFV77W4": "Apple",
    "B0B1NGPY94": "Google",
    "B0BYSQDWRT": "Shokz",
    "B0B2SH4CN6": "Samsung",
    "B09D1HMBQ3": "Jabra",
    "B0D4STD5ZC": "Beats",
    "B099TLMRB6": "Beats",
    "B07GWRCZQP": "JBL",
    "B09GK5JMHK": "Sony",
    "B088KRKFJ3": "Sennheiser",
    "B08Z1RP9K8": "Anker",
    "B0B445JCZ3": "Sony",
    "B0B43Y8GHZ": "Sony"
}

df['brand'] = df['product_id'].map(brand_map)

df.to_csv('cleaned_reviews.csv', index=False)

from google.colab import files
files.download("cleaned_reviews.csv")

# average rating by brand

plt.figure(figsize=(12, 6))
sns.barplot(x='brand', y='rating', data=df, palette='viridis')

plt.title("Average Rating by Brand")
plt.xlabel("Brand")
plt.ylabel("Average Rating")
plt.show()

beats_average_rating = df[df['brand'] == 'Beats']['rating'].mean()
print(f"The average rating for Beats products is: {beats_average_rating:.2f}")

total_reviews = len(df)
print(f"Total number of reviews analyzed: {total_reviews}")

overall_average_rating = df['rating'].mean()
print(f"The overall average rating is: {overall_average_rating:.2f}")

# average helpful votes per brand

brand_helpful = df.groupby('brand')['helpful_count'].mean().sort_values(ascending=False)

plt.figure(figsize=(12, 6))
sns.barplot(x=brand_helpful.index, y=brand_helpful.values, palette='magma')

plt.title("Average Helpful Votes per Brand")
plt.xlabel("Brand")
plt.ylabel("Average Helpful Votes")
plt.xticks(rotation=45)
plt.show()

"""Now! Let's calculate the correlation matrix for all this"""

import seaborn as sns
import matplotlib.pyplot as plt

df['rating'] = pd.to_numeric(df['rating'], errors='coerce')

from scipy import stats

# Create a list of rating arrays for each brand
brand_ratings = [df['rating'][df['brand'] == brand].dropna() for brand in df['brand'].unique()]

# Perform one-way ANOVA test
f_statistic, p_value = stats.f_oneway(*brand_ratings)

print(f"ANOVA F-statistic: {f_statistic:.2f}")
print(f"ANOVA p-value: {p_value:.4f}")

# Interpret the results
alpha = 0.05
if p_value < alpha:
    print("The p-value is less than the significance level (alpha), so we reject the null hypothesis.")
    print("There are significant differences in the average ratings across different brands.")
else:
    print("The p-value is greater than the significance level (alpha), so we fail to reject the null hypothesis.")
    print("There are no significant differences in the average ratings across different brands.")

# correlation matrix
correlation = df[['rating', 'helpful_count', 'is_verified']].corr()

display(correlation)

plt.figure(figsize=(8, 6))
sns.heatmap(correlation, annot=True, cmap='coolwarm', fmt=".2f")
plt.title=("Heatmap of the matrix")
plt.show()

beats_reviews = df[df['brand'] == 'Beats']

plt.figure(figsize=(8,6))
sns.countplot(x='rating', data=beats_reviews, palette='viridis')
plt.title=("Distribution of Ratings for Beats Speakers")
plt.xlabel("Rating")
plt.ylabel("Count")
plt.show()

# converting date to numerical (months since earliest review)
df['month_since_stard'] = (df['year_month'].dt.to_timestamp() - df['year_month'].dt.to_timestamp().min()).dt.days // 30 # approximate months

# correlation matrix with new column
correlation_matrix = df[['rating', 'helpful_count', 'is_verified', 'month_since_stard']].corr()

# display the correlation matrix
display(correlation_matrix)

# heatmap
plt.figure(figsize=(10, 8))
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=".2f")
plt.title=("Correlation Matrix")
plt.show()